{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3b194c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61a029f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Downloads/trainMSCI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bec4467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Downloads/testMSCI2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a948071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>isRelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ferguson riots pregnant woman loses eye cops f...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>respected senior french police officer investi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crazy conservatives sure gitmo detainee killed...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>dave morins social networking company path rep...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>russian guy says justin bieber ringtone saved ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>bereaved afghan mother took revenge taliban wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zombie cat buried kitty believed dead meows ba...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>hewlettpackard officially splitting two follow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>argentinas president adopts boy end werewolf c...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>airline passenger headed dallas removed plane ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25408</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2582</td>\n",
       "      <td>agree</td>\n",
       "      <td>congressional republicans evidently hoping rep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25409</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2583</td>\n",
       "      <td>discuss</td>\n",
       "      <td>obamacare work worth reflecting upon president...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25410</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2584</td>\n",
       "      <td>disagree</td>\n",
       "      <td>millions may lose coverage next year congress ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25411</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2585</td>\n",
       "      <td>disagree</td>\n",
       "      <td>come november grim trudge across increasingly ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25412</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2586</td>\n",
       "      <td>agree</td>\n",
       "      <td>remember much republicans wanted repeal obamac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25413 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Body ID     Stance  \\\n",
       "0      ferguson riots pregnant woman loses eye cops f...     2008  unrelated   \n",
       "1      crazy conservatives sure gitmo detainee killed...     1550  unrelated   \n",
       "2      russian guy says justin bieber ringtone saved ...        2  unrelated   \n",
       "3      zombie cat buried kitty believed dead meows ba...     1793  unrelated   \n",
       "4      argentinas president adopts boy end werewolf c...       37  unrelated   \n",
       "...                                                  ...      ...        ...   \n",
       "25408  success affordable care act hugely inconvenien...     2582      agree   \n",
       "25409  success affordable care act hugely inconvenien...     2583    discuss   \n",
       "25410  success affordable care act hugely inconvenien...     2584   disagree   \n",
       "25411  success affordable care act hugely inconvenien...     2585   disagree   \n",
       "25412  success affordable care act hugely inconvenien...     2586      agree   \n",
       "\n",
       "                                             articleBody  isRelated  \n",
       "0      respected senior french police officer investi...          0  \n",
       "1      dave morins social networking company path rep...          0  \n",
       "2      bereaved afghan mother took revenge taliban wa...          0  \n",
       "3      hewlettpackard officially splitting two follow...          0  \n",
       "4      airline passenger headed dallas removed plane ...          0  \n",
       "...                                                  ...        ...  \n",
       "25408  congressional republicans evidently hoping rep...          1  \n",
       "25409  obamacare work worth reflecting upon president...          1  \n",
       "25410  millions may lose coverage next year congress ...          1  \n",
       "25411  come november grim trudge across increasingly ...          1  \n",
       "25412  remember much republicans wanted repeal obamac...          1  \n",
       "\n",
       "[25413 rows x 5 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "efd8d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3307 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 3500\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer1 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer1.fit_on_texts(train['Headline'].values)\n",
    "word_index = tokenizer1.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "567e01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (49972, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train_headlines = tokenizer1.texts_to_sequences(train['Headline'].values)\n",
    "X_train_headlines = pad_sequences(X_train_headlines, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_train_headlines.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d5d5d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25413, 100)\n"
     ]
    }
   ],
   "source": [
    "X_test_headlines = tokenizer1.texts_to_sequences(test['Headline'].values)\n",
    "X_test_headlines = pad_sequences(X_test_headlines, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_test_headlines.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "530115f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24783 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer2 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer2.fit_on_texts(train['articleBody'].values)\n",
    "word_index = tokenizer2.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aaa411ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (49972, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_body = tokenizer2.texts_to_sequences(train['articleBody'].values)\n",
    "X_train_body = pad_sequences(X_train_body, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_train_body.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "92bc6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25413, 100)\n"
     ]
    }
   ],
   "source": [
    "X_test_body = tokenizer2.texts_to_sequences(test['articleBody'].values)\n",
    "X_test_body = pad_sequences(X_test_body, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_test_body.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e225e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (49972, 2)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies( train['isRelated']).values\n",
    "print('Shape of label tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89efd67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (25413, 2)\n"
     ]
    }
   ],
   "source": [
    "Y_test = pd.get_dummies( test['isRelated']).values\n",
    "print('Shape of label tensor:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d45e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import LSTM, Dense, InputLayer, Concatenate, Input\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1c1dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1_input = Input(X_train_headlines.shape[1])\n",
    "column2_input = Input(X_train_body.shape[1])\n",
    "\n",
    "# column1_LSTM\n",
    "c0 = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_headlines.shape[1])(column1_input)\n",
    "c1 = LSTM(64, return_sequences=False)(c0)\n",
    "c2 = Dense(64)(c1)\n",
    "\n",
    "# column2_LSTM\n",
    "d0 = (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_body.shape[1]))(column2_input)\n",
    "d1 = LSTM(64, return_sequences=False)(d0)\n",
    "d2 = Dense(64)(d1)\n",
    "\n",
    "# shared\n",
    "concat_layer= Concatenate()([c2, d2])\n",
    "shared1 = Dense(128, activation='relu')(concat_layer)\n",
    "output = Dense(2, activation='sigmoid')(shared1)\n",
    "\n",
    "# define a model with a list of two inputs\n",
    "model = Model(inputs=[column1_input, column2_input], outputs=output)\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8186b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Total params: 709,570\n",
      "Trainable params: 709,570\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86c2972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 81s 100ms/step - loss: 0.4856\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 77s 99ms/step - loss: 0.4144\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 78s 100ms/step - loss: 0.3929\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 80s 103ms/step - loss: 0.3853\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 88s 112ms/step - loss: 0.3808\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit([X_train_headlines,X_train_body ], Y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78a6b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test_headlines,X_test_body ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b3be12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32616723, 0.6698896 ],\n",
       "       [0.526286  , 0.4759713 ],\n",
       "       [0.7264933 , 0.2725665 ],\n",
       "       ...,\n",
       "       [0.62370485, 0.37320465],\n",
       "       [0.69233096, 0.3082329 ],\n",
       "       [0.7433839 , 0.25685298]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb715014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0acb6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['isRelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6698a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.23841241502437682\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69     18349\n",
      "           1       0.23      0.25      0.24      7064\n",
      "\n",
      "    accuracy                           0.56     25413\n",
      "   macro avg       0.47      0.47      0.47     25413\n",
      "weighted avg       0.57      0.56      0.57     25413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "\n",
    "\n",
    "f1 = f1_score(y_true,preds)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f99d4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retraining with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afc4f07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hamzakazmi/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 81s 100ms/step - loss: 0.7063 - accuracy: 0.7644\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 73s 94ms/step - loss: 0.6122 - accuracy: 0.8050\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.5914 - accuracy: 0.8100\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 75s 96ms/step - loss: 0.5806 - accuracy: 0.8142\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 78s 99ms/step - loss: 0.5757 - accuracy: 0.8178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_weight = {0: 1.,\n",
    "                1: 2.}\n",
    "history = model.fit([X_train_headlines,X_train_body ], Y_train,class_weight =class_weight, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d69e2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test_headlines,X_test_body ])\n",
    "preds = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "faafa5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.34606133493686114\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.49      0.58     18349\n",
      "           1       0.27      0.49      0.35      7064\n",
      "\n",
      "    accuracy                           0.49     25413\n",
      "   macro avg       0.49      0.49      0.46     25413\n",
      "weighted avg       0.59      0.49      0.51     25413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true,preds)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2613b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we see improvement in predicting class 1, and therefore as class 1 has higher weightage, using class weights is good approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4d89a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we try to use only one lstm model to predict all 4 classes rather than the two stage appraoch\n",
    "from keras.layers import SpatialDropout1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8da63fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1_input = Input(X_train_headlines.shape[1])\n",
    "column2_input = Input(X_train_body.shape[1])\n",
    "\n",
    "# column1_LSTM\n",
    "c0 = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_headlines.shape[1])(column1_input)\n",
    "c1 = SpatialDropout1D(0.2)(c0)\n",
    "c2 = LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(c1)\n",
    "c3 = Dense(64)(c2)\n",
    "\n",
    "# column2_LSTM\n",
    "d0 = (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_body.shape[1]))(column2_input)\n",
    "d1 = SpatialDropout1D(0.2)(d0)\n",
    "d2 = LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(d1)\n",
    "d3 = Dense(64)(d2)\n",
    "\n",
    "# shared\n",
    "concat_layer= Concatenate()([c3, d3])\n",
    "shared1 = Dense(64, activation='relu')(concat_layer)\n",
    "output = Dense(4, activation='softmax')(shared1)\n",
    "\n",
    "# define a model with a list of two inputs\n",
    "model = Model(inputs=[column1_input, column2_input], outputs=output)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "611f344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (49972, 4)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies( train['Stance']).values\n",
    "print('Shape of label tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b70e2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (25413, 4)\n"
     ]
    }
   ],
   "source": [
    "Y_test = pd.get_dummies( test['Stance']).values\n",
    "print('Shape of label tensor:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "94054999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrelated    18349\n",
       "discuss       4464\n",
       "agree         1903\n",
       "disagree       697\n",
       "Name: Stance, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "67d2acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 229s 288ms/step - loss: 1.1765 - accuracy: 0.7502\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 276s 354ms/step - loss: 0.9732 - accuracy: 0.7738\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 298s 381ms/step - loss: 0.9111 - accuracy: 0.7845\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 300s 384ms/step - loss: 0.8778 - accuracy: 0.7904\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 303s 388ms/step - loss: 0.8617 - accuracy: 0.7911\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 2.5,\n",
    "                1: 3,\n",
    "                2: 2,\n",
    "                3: 1}\n",
    "history = model.fit([X_train_headlines,X_train_body ], Y_train,class_weight =class_weight, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b6fcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test_headlines,X_test_body ])\n",
    "preds = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f891f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unrelated = 3\n",
    "#agree = 0\n",
    "#discuss =2\n",
    "#disagree = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "121bda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['true'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "16395051",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.Stance == 'unrelated', 'true'] = 3\n",
    "test.loc[test.Stance == 'discuss', 'true'] = 2\n",
    "test.loc[test.Stance == 'agree', 'true'] = 0\n",
    "test.loc[test.Stance == 'disagree', 'true'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ebc6fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d1c92eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.19      0.12      1903\n",
      "           1       0.00      0.00      0.00       697\n",
      "           2       0.19      0.31      0.24      4464\n",
      "           3       0.72      0.55      0.62     18349\n",
      "\n",
      "    accuracy                           0.47     25413\n",
      "   macro avg       0.25      0.26      0.25     25413\n",
      "weighted avg       0.56      0.47      0.50     25413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_true,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6bc97c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "049cb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.pred == 3, 'Stance'] = 'unrelated'\n",
    "test.loc[test.pred == 2, 'Stance'] = 'discuss'\n",
    "test.loc[test.pred == 0, 'Stance'] = 'agree'\n",
    "test.loc[test.pred == 1, 'Stance'] = 'disagree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c0655b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Headline','Body ID', 'Stance']].to_csv('Desktop/answer.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3e9fe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on results we think 2 stage approach is better and below we continue with the two stage appraoch and build the\n",
    "#second model\n",
    "\n",
    "#This is because of data unblancing discussed more in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0de3ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 predict related type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f41113f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.loc[train['isRelated'] ==1]\n",
    "test2 = test.loc[test['isRelated'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad97b7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3307 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer3 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer3.fit_on_texts(train2['Headline'].values)\n",
    "word_index = tokenizer3.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7bcfde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (13427, 100)\n",
      "Shape of data tensor: (7064, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_headlines = tokenizer3.texts_to_sequences(train2['Headline'].values)\n",
    "X_train_headlines = pad_sequences(X_train_headlines, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_train_headlines.shape)\n",
    "\n",
    "X_test_headlines = tokenizer3.texts_to_sequences(test2['Headline'].values)\n",
    "X_test_headlines = pad_sequences(X_test_headlines, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_test_headlines.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9568c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24783 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer4 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer4.fit_on_texts(train2['articleBody'].values)\n",
    "word_index = tokenizer4.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3040f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (13427, 100)\n",
      "Shape of data tensor: (7064, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_body = tokenizer4.texts_to_sequences(train2['articleBody'].values)\n",
    "X_train_body = pad_sequences(X_train_body, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_train_body.shape)\n",
    "X_test_body = tokenizer4.texts_to_sequences(test2['articleBody'].values)\n",
    "X_test_body = pad_sequences(X_test_body, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print('Shape of data tensor:', X_test_body.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c094808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (13427, 3)\n",
      "Shape of label tensor: (7064, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies( train2['Stance']).values\n",
    "print('Shape of label tensor:', Y_train.shape)\n",
    "\n",
    "Y_test = pd.get_dummies( test2['Stance']).values\n",
    "print('Shape of label tensor:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4a2d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1_input = Input(X_train_headlines.shape[1])\n",
    "column2_input = Input(X_train_body.shape[1])\n",
    "\n",
    "# column1_LSTM\n",
    "c0 = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_headlines.shape[1])(column1_input)\n",
    "c1 = LSTM(64, return_sequences=False)(c0)\n",
    "c2 = Dense(64)(c1)\n",
    "\n",
    "# column2_LSTM\n",
    "d0 = (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_body.shape[1]))(column2_input)\n",
    "d1 = LSTM(64, return_sequences=False)(d0)\n",
    "d2 = Dense(64)(d1)\n",
    "\n",
    "# shared\n",
    "concat_layer= Concatenate()([c2, d2])\n",
    "shared1 = Dense(128, activation='relu')(concat_layer)\n",
    "output = Dense(3, activation='softmax')(shared1)\n",
    "\n",
    "# define a model with a list of two inputs\n",
    "model2 = Model(inputs=[column1_input, column2_input], outputs=output)\n",
    "model2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34ddb6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "210/210 [==============================] - 23s 96ms/step - loss: 0.6428 - accuracy: 0.7258\n",
      "Epoch 2/5\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.4619 - accuracy: 0.8090\n",
      "Epoch 3/5\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.4084 - accuracy: 0.8256\n",
      "Epoch 4/5\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.3980 - accuracy: 0.8311\n",
      "Epoch 5/5\n",
      "210/210 [==============================] - 22s 104ms/step - loss: 0.3978 - accuracy: 0.8258\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit([X_train_headlines,X_train_body ], Y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e816b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict([X_test_headlines,X_test_body ])\n",
    "preds = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9cfd86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "993e956e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>isRelated</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>exclusive apple unveil longawaited retina macb...</td>\n",
       "      <td>1964</td>\n",
       "      <td>agree</td>\n",
       "      <td>last week apple sent invites spring forward ev...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>found missing afghan soldiers spotted trying e...</td>\n",
       "      <td>2312</td>\n",
       "      <td>discuss</td>\n",
       "      <td>three afghanistan national army officers disap...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>report three missing afghan soldiers caught ca...</td>\n",
       "      <td>1754</td>\n",
       "      <td>discuss</td>\n",
       "      <td>toronto three missing afghan soldiers taken cu...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>boobed woman fake</td>\n",
       "      <td>1618</td>\n",
       "      <td>agree</td>\n",
       "      <td>woman claiming third breast play hoax us snope...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>isis might harvesting organs iraqi ambassador ...</td>\n",
       "      <td>930</td>\n",
       "      <td>discuss</td>\n",
       "      <td>isis using blood money harvesting organs fund ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25408</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2582</td>\n",
       "      <td>agree</td>\n",
       "      <td>congressional republicans evidently hoping rep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25409</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2583</td>\n",
       "      <td>discuss</td>\n",
       "      <td>obamacare work worth reflecting upon president...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25410</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2584</td>\n",
       "      <td>disagree</td>\n",
       "      <td>millions may lose coverage next year congress ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25411</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2585</td>\n",
       "      <td>disagree</td>\n",
       "      <td>come november grim trudge across increasingly ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25412</th>\n",
       "      <td>success affordable care act hugely inconvenien...</td>\n",
       "      <td>2586</td>\n",
       "      <td>agree</td>\n",
       "      <td>remember much republicans wanted repeal obamac...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7064 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Body ID    Stance  \\\n",
       "10     exclusive apple unveil longawaited retina macb...     1964     agree   \n",
       "12     found missing afghan soldiers spotted trying e...     2312   discuss   \n",
       "14     report three missing afghan soldiers caught ca...     1754   discuss   \n",
       "15                                     boobed woman fake     1618     agree   \n",
       "20     isis might harvesting organs iraqi ambassador ...      930   discuss   \n",
       "...                                                  ...      ...       ...   \n",
       "25408  success affordable care act hugely inconvenien...     2582     agree   \n",
       "25409  success affordable care act hugely inconvenien...     2583   discuss   \n",
       "25410  success affordable care act hugely inconvenien...     2584  disagree   \n",
       "25411  success affordable care act hugely inconvenien...     2585  disagree   \n",
       "25412  success affordable care act hugely inconvenien...     2586     agree   \n",
       "\n",
       "                                             articleBody  isRelated  true  \n",
       "10     last week apple sent invites spring forward ev...          1     1  \n",
       "12     three afghanistan national army officers disap...          2     2  \n",
       "14     toronto three missing afghan soldiers taken cu...          2     2  \n",
       "15     woman claiming third breast play hoax us snope...          1     1  \n",
       "20     isis using blood money harvesting organs fund ...          2     2  \n",
       "...                                                  ...        ...   ...  \n",
       "25408  congressional republicans evidently hoping rep...          1     1  \n",
       "25409  obamacare work worth reflecting upon president...          2     2  \n",
       "25410  millions may lose coverage next year congress ...          1     0  \n",
       "25411  come november grim trudge across increasingly ...          1     0  \n",
       "25412  remember much republicans wanted repeal obamac...          1     1  \n",
       "\n",
       "[7064 rows x 6 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agree = 0\n",
    "#discuss =2\n",
    "#disagree 1\n",
    "\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d168801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2['true'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4320bd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discuss     4464\n",
       "agree       1903\n",
       "disagree     697\n",
       "Name: Stance, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2['Stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98232cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.loc[test2.Stance == 'agree', 'true'] = 0\n",
    "test2.loc[test2.Stance == 'discuss', 'true'] = 2\n",
    "test2.loc[test2.Stance == 'agree', 'true'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96e36a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.53      0.20       697\n",
      "           1       0.36      0.00      0.00      1903\n",
      "           2       0.74      0.69      0.71      4464\n",
      "\n",
      "    accuracy                           0.49      7064\n",
      "   macro avg       0.41      0.41      0.31      7064\n",
      "weighted avg       0.58      0.49      0.47      7064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(test2['true'],preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24bb48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f77fc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "210/210 [==============================] - 24s 107ms/step - loss: 0.6174 - accuracy: 0.8197\n",
      "Epoch 2/5\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.5700 - accuracy: 0.8252\n",
      "Epoch 3/5\n",
      "210/210 [==============================] - 20s 94ms/step - loss: 0.5730 - accuracy: 0.8246\n",
      "Epoch 4/5\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.5455 - accuracy: 0.8236\n",
      "Epoch 5/5\n",
      "210/210 [==============================] - 20s 94ms/step - loss: 0.5508 - accuracy: 0.8216\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.7,\n",
    "                1: 2.7,\n",
    "               2: 1.}\n",
    "\n",
    "history = model2.fit([X_train_headlines,X_train_body ], Y_train,class_weight=class_weight, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4164f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict([X_test_headlines,X_test_body ])\n",
    "preds = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be4d6ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.67      0.22       697\n",
      "           1       0.28      0.07      0.12      1903\n",
      "           2       0.78      0.52      0.63      4464\n",
      "\n",
      "    accuracy                           0.42      7064\n",
      "   macro avg       0.40      0.42      0.32      7064\n",
      "weighted avg       0.58      0.42      0.45      7064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(test2['true'],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
